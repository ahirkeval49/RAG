import os
import streamlit as st
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from huggingface_hub import login

# ----------------------------------
# 1) Hugging Face Token Management
# ----------------------------------
# Option A (Recommended): Use environment variable
hf_token = os.environ.get("HUGGINGFACEHUB_API_TOKEN")

# If you prefer Option B (Inline), uncomment below and paste your new token:
# hf_token = "hf_newTokenExample123"  # <-- Not recommended for production

if not hf_token:
    st.warning("No Hugging Face token found. Please set HUGGINGFACEHUB_API_TOKEN or inline the token.")
else:
    login(token=hf_token)


# ----------------------------------
# 2) Utility Functions
# ----------------------------------

def load_and_chunk_documents(data_path: str, chunk_size: int = 1000, chunk_overlap: int = 50):
    """
    Load PDFs from the data directory, convert to text, and chunk them.
    """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    all_chunks = []

    for file_name in os.listdir(data_path):
        if file_name.lower().endswith(".pdf"):
            pdf_path = os.path.join(data_path, file_name)
            loader = PyPDFLoader(pdf_path)
            pages = loader.load()
            for page in pages:
                # Split each PDF page into smaller chunks
                chunks = text_splitter.split_text(page.page_content)
                all_chunks.extend(chunks)

    return all_chunks


def build_vectorstore(chunks, persist_directory="./chroma_db"):
    """
    Convert text chunks into embeddings using a Hugging Face model,
    and store them in a Chroma vector database.
    """
    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = Chroma.from_texts(
        texts=chunks,
        embedding=embedder,
        collection_name="my_collection",
        persist_directory=persist_directory
    )
    vectordb.persist()
    return vectordb


def load_vectorstore(persist_directory="./chroma_db"):
    """
    Load an existing Chroma vector database if it has been persisted before.
    """
    embedder = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = Chroma(
        collection_name="my_collection",
        embedding_function=embedder,
        persist_directory=persist_directory
    )
    return vectordb


def retrieve_relevant_docs(vectorstore, user_query, k=3):
    """
    Retrieve top-k most relevant chunks from the vector store based on user_query.
    """
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": k})
    return retriever.get_relevant_documents(user_query)


def load_hf_llm(model_name="tiiuae/falcon-7b-instruct"):
    """
    Load a Hugging Face instruct/chat model for text generation.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",  # automatically use GPU if available
        torch_dtype=torch.float16  # use half precision to save GPU memory
    )
    return tokenizer, model


def generate_response(prompt, tokenizer, model):
    """
    Generate text from the LLM using the provided prompt.
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def rag_chatbot_pipeline(user_query, vectorstore, tokenizer, model):
    """
    Combine retrieval + LLM to produce an answer grounded in the retrieved text chunks.
    """
    docs = retrieve_relevant_docs(vectorstore, user_query, k=3)
    context_str = "\n\n".join([doc.page_content for doc in docs])

    # Prompt to guide the model to use only provided context
    prompt = f"""
    You are a helpful assistant. Use the following context to answer the user's question.
    If the answer is not in the context, say "I am not sure."

    CONTEXT:
    {context_str}

    QUESTION:
    {user_query}

    ANSWER:
    """
    answer = generate_response(prompt, tokenizer, model)
    return answer


# ----------------------------------
# 3) Streamlit App
# ----------------------------------

def main():
    st.title("RAG-based Chatbot (Streamlit)")

    # Sidebar - Data and Vector Store Management
    data_dir = st.sidebar.text_input("Data Directory", value="data", help="Folder containing your PDF documents.")
    build_db_button = st.sidebar.button("Build or Rebuild Vector Store")

    # If user clicks "Build or Rebuild Vector Store"
    if build_db_button:
        st.sidebar.write("Building the vector store from PDFs...")
        chunks = load_and_chunk_documents(data_dir)
        vectordb = build_vectorstore(chunks, "./chroma_db")
        st.sidebar.success("Vector store built successfully!")
        st.session_state["vectordb"] = vectordb

    # Ensure we have a vector store loaded
    if "vectordb" not in st.session_state:
        if os.path.exists("./chroma_db"):
            st.session_state["vectordb"] = load_vectorstore("./chroma_db")
        else:
            st.warning(
                "No vector store found. Place PDFs in 'data/' and click 'Build or Rebuild Vector Store' in the sidebar.")
            return

    # Load or cache the Hugging Face model in session state
    if "tokenizer" not in st.session_state or "model" not in st.session_state:
        with st.spinner("Loading LLM (this might take some time)..."):
            tokenizer, model = load_hf_llm("tiiuae/falcon-7b-instruct")
        st.session_state["tokenizer"] = tokenizer
        st.session_state["model"] = model
        st.success("LLM loaded!")

    # Initialize chat history
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = []  # list of (user_msg, assistant_msg)

    # Display existing chat
    for i, (user_text, bot_text) in enumerate(st.session_state["chat_history"]):
        st.chat_message("user", avatar="ðŸ‘¤", key=f"user_msg_{i}").write(user_text)
        st.chat_message("assistant", avatar="ðŸ¤–", key=f"bot_msg_{i}").write(bot_text)

    # Chat input
    user_query = st.chat_input("Ask a question about your documents...")

    if user_query:
        # Display user message immediately
        st.chat_message("user", avatar="ðŸ‘¤").write(user_query)

        # Get the Vector DB, Model, Tokenizer from session state
        vectordb = st.session_state["vectordb"]
        tokenizer = st.session_state["tokenizer"]
        model = st.session_state["model"]

        # Generate response
        with st.chat_message("assistant", avatar="ðŸ¤–"):
            with st.spinner("Thinking..."):
                bot_response = rag_chatbot_pipeline(user_query, vectordb, tokenizer, model)
                st.write(bot_response)

        # Append to chat history
        st.session_state["chat_history"].append((user_query, bot_response))


if __name__ == "__main__":
    main()
